{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe1551e",
   "metadata": {},
   "source": [
    "# Bills TF-IDF\n",
    "\n",
    "This Python notebook visualizes the TF-IDF analysis done to the bills in the 18th Congress of House of Representatives\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "Make sure that you have the following libraries installed on your machine before running the cells below to avoid any errors.\n",
    "- `pandas`\n",
    "- `nltk`\n",
    "- `tqdm`\n",
    "- `sklearn`\n",
    "\n",
    "All libraries were installed via `pip` using the command: `pip install <library-name>`. If you have `pip` installed in your machine, then you can easily install the following libraries using the command shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8f338",
   "metadata": {},
   "source": [
    "## PART 1: Upload the Dataframe\n",
    "\n",
    "Upload the CSV file as a pandas Data Frame and print some entries to verify that the file was uploaded and read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a4efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('18th_hor_bills_dataset_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06466ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                                                                HB00002\n",
       "Full Title                              AN ACT CREATING THE DEPARTMENT OF OVERSEAS FIL...\n",
       "Author Count                                                                            3\n",
       "is_partylist                                                                            0\n",
       "party_1-pacman                                                                          0\n",
       "                                                              ...                        \n",
       "ref_defeat_covid-19_ad-hoc_committee                                                    0\n",
       "ref_the_whole_house                                                                     0\n",
       "ref_mindanao_affairs                                                                    0\n",
       "ref_west_philippine_sea                                                                 0\n",
       "approved                                                                                1\n",
       "Name: 1, Length: 279, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cf6cb",
   "metadata": {},
   "source": [
    "# PART 2: Load the Stop Words and Extract the Bill Title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067d009",
   "metadata": {},
   "source": [
    "A list of English stopwords can be loaded using the `nltk` library. We can download the `stopwords` first before loading it to a variable `stops`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a232892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\james\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this cell only if 'stopwords' has not been downloaded or if the succeeding cell throws an error\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e76d652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f565e54",
   "metadata": {},
   "source": [
    "Extract the 'Full Title' of the bills first into a separate dataset to prepare it for TF-IDF analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3302a736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    AN ACT INSTITUTIONALIZING A NATIONAL VALUES, E...\n",
       "1    AN ACT CREATING THE DEPARTMENT OF OVERSEAS FIL...\n",
       "2    AN ACT PROVIDING FOR A NATIONAL PROGRAM TO SUP...\n",
       "3    AN ACT CREATING THE EMERGENCY RESPONSE DEPARTM...\n",
       "4    AN ACT INSTITUTIONALIZING MICROFINANCE PROGRAM...\n",
       "Name: Full Title, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_title = df['Full Title'].copy(deep = True)\n",
    "df_full_title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24581192",
   "metadata": {},
   "source": [
    "As we prepare the list of bills for TF-IDF, we can now remove the stop words and punctuations from each of the full titles.\n",
    "\n",
    "Let us define a function `remove_punctuation` that removes the punctuations and retains the letters from each of the token, and test the function over a set of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d95e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(token):\n",
    "    import string\n",
    "    return token.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b35bb9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shout out to my ex you are really quite the man you made my heart break and that made me who I am'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str_1 = 'Shout out to my ex, you are really quite the man; you made my heart break, and that made me who I am'\n",
    "remove_punctuation(test_str_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb77dc",
   "metadata": {},
   "source": [
    "Let us define a function `remove_stop_words` that does the following:\n",
    "<ul>\n",
    "    <li> Split the bill into a list of tokens or words </li>\n",
    "    <li> If a stop word, as indicated in variable a defined above, is included, remove them from the list of tokens </li>\n",
    "    <li> Combine the remaining words into a string separated by spaces </li>\n",
    "</ul>\n",
    "We can then test the newly created function to some test entries, before finally iterating it over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aae8d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(bill_string, stopwords):\n",
    "    lis = bill_string.split() #split the string according to spaces\n",
    "    to_return = [] #define a new list of words\n",
    "    \n",
    "    for i in lis:\n",
    "        if i.lower() not in stopwords:\n",
    "            to_return.append(i)\n",
    "    \n",
    "    return remove_punctuation(\" \".join(to_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a0f4c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACT CREATING DEPARTMENT OVERSEAS FILIPINO WORKERS OFW FOREIGN EMPLOYMENT DEFINING POWERS FUNCTIONS APPROPRIATING FUNDS THEREFOR RATIONALIZING ORGANIZATION FUNCTIONS GOVERNMENT AGENCIES RELATED MIGRATION PURPOSES'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stop_words(df_full_title.iloc[1], stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c34557e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACT PROVIDING COMPREHENSIVE CIVIL REGISTRATION SYSTEM'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stop_words(df_full_title.iloc[103], stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cffb20",
   "metadata": {},
   "source": [
    "Upon running the function initially over all bills, the function threw an error over at entry 9278, which corresponds to 'HB09290'. The full title of this house bill is an empty string. Upon cross-checking the data with the website, it was found out that the website was not able to include the full title of the actual bill. Fortunately, the full title of this bill can be accessed using the link: https://hrep-website.s3.ap-southeast-1.amazonaws.com/legisdocs/basic_18/HB09290.pdf\n",
    "\n",
    "The name of the bill will then be hardcoded to its corresponding entry, 9278, before running the function over all bills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04027219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AN ACT TO IMPROVE ACCESS TO PRESCHOOL, PRIMARY, AND SECONDARY EDUCATION OF HOMELESS CHILDREN AND YOUTH'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_title.iloc[9278] = \"AN ACT TO IMPROVE ACCESS TO PRESCHOOL, PRIMARY, AND SECONDARY EDUCATION OF HOMELESS CHILDREN AND YOUTH\"\n",
    "df_full_title.iloc[9278]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9101a5df",
   "metadata": {},
   "source": [
    "The function will be applied to all bills, and test the new data by sampling entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78486726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c213ebad5f44defa6b18b459889fc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for i in tqdm(range(len(df_full_title))):\n",
    "    df_full_title.iloc[i] = remove_stop_words(df_full_title.iloc[i], stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a80490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACT IMPROVE ACCESS PRESCHOOL PRIMARY SECONDARY EDUCATION HOMELESS CHILDREN YOUTH'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_title.iloc[9278]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50ed187c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACT ESTABLISHING BENHAM RISE RESEARCH DEVELOPMENT INSTITUTE PROVIDING FUNDS THEREFOR PURPOSES'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_title.iloc[34]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378131a",
   "metadata": {},
   "source": [
    "# PART 3: Extract the Bag of Words\n",
    "\n",
    "A <i>bag of words</i> is then defined as a set of unique words generated from the list of full titles. This bag of words can then be extracted according to a specified number of `n`-grams. \n",
    "\n",
    "Let us define a dictionary of `n`-grams that can be initialized over a range. These can be tweaked whenever we want to adjust the range of `n`-grams that we want to extract from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a2102e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can tweak lower_limit and upper_limit depending on the number of n-grams that you want to be extracted\n",
    "lower_limit = 2\n",
    "upper_limit = 3\n",
    "\n",
    "bag_of_words_per_n_gram = {x: [] for x in range(lower_limit, upper_limit + 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f026ebcc",
   "metadata": {},
   "source": [
    "Let us define a function `extract_n_grams` that extracts `n`-grams from a particular string, and then appending it to the variable `bag_of_words_per_n_gram`, which is a dictionary of `n`-grams, depending on the set lower limit and upper limit (given by `lower_limit` and `upper_limit` variables, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "919c8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def extract_n_grams(title, bag, lower, upper):\n",
    "    '''a function that extracts n-grams from the lower to the upper limit.\n",
    "    Parameters of the function include:\n",
    "    =>title - input string to be extracted\n",
    "    =>bag - the bag of words\n",
    "    =>lower - lower limit\n",
    "    =>upper - upper limit'''\n",
    "    \n",
    "    spl = title.split()\n",
    "    for i in range(lower, upper+1):\n",
    "        temp = set(ngrams(spl, i))\n",
    "        bag[i] = list(set(bag[i]).union(temp))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8778e1f1",
   "metadata": {},
   "source": [
    "We now iterate the function over all bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b14e030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in tqdm(range(len(df_full_title))):\\n    extract_n_grams(df_full_title.iloc[i], bag_of_words_per_n_gram, lower_limit, upper_limit)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in tqdm(range(len(df_full_title))):\n",
    "    extract_n_grams(df_full_title.iloc[i], bag_of_words_per_n_gram, lower_limit, upper_limit)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f76fab",
   "metadata": {},
   "source": [
    "To check how many unique `n`-grams identified in the range, you may run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caf5c0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(lower_limit, upper_limit + 1):\\n    bag_of_words_per_n_gram[i] = list(map(lambda x: \" \".join(x), bag_of_words_per_n_gram[i]))\\n    print(\"There are {} unique {}-grams identified\".format(len(bag_of_words_per_n_gram[i]), i))'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in range(lower_limit, upper_limit + 1):\n",
    "    bag_of_words_per_n_gram[i] = list(map(lambda x: \" \".join(x), bag_of_words_per_n_gram[i]))\n",
    "    print(\"There are {} unique {}-grams identified\".format(len(bag_of_words_per_n_gram[i]), i))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2487ab4b",
   "metadata": {},
   "source": [
    "# PART 4: Perform TF-IDF\n",
    "\n",
    "For this portion, we will be using the `TfidfVectorizer` function from `sklearn.feature_extraction.text`. This function needs the following input parameters:\n",
    "- `vocabulary`: The bag of words extracted in the previous part, given by `bag_of_words_per_n_gram`\n",
    "- `stop-words`\n",
    "- `ngram_range`: The range of n-grams, given by the `lower_limit` and `upper_limit`\n",
    "\n",
    "Before calling `TfidfVectorizer`, we need to combine the dictionary `bag_of_words_per_n_gram` into a single list to feed into the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69fe46f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvocabs = []\\n\\nfor i in range(lower_limit, upper_limit+1):\\n    vocabs += list(set(bag_of_words_per_n_gram[i]))\\n\\nvocabs = [i.lower() for i in vocabs]\\n\\n#transform it into a set to ensure that there are no duplicates\\nvocabs = list(set(vocabs))\\n\\nprint(len(vocabs))\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "vocabs = []\n",
    "\n",
    "for i in range(lower_limit, upper_limit+1):\n",
    "    vocabs += list(set(bag_of_words_per_n_gram[i]))\n",
    "\n",
    "vocabs = [i.lower() for i in vocabs]\n",
    "\n",
    "#transform it into a set to ensure that there are no duplicates\n",
    "vocabs = list(set(vocabs))\n",
    "\n",
    "print(len(vocabs))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344113c",
   "metadata": {},
   "source": [
    "Perform the actual TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3e85079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=3000, stop_words = 'english', ngram_range=(lower_limit, upper_limit))\n",
    "tfs = tfidf.fit_transform(df_full_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c964aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names_out()\n",
    "corpus_index = [n for n in range(len(df_full_title))]\n",
    "rows, cols = tfs.nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199dfce6",
   "metadata": {},
   "source": [
    "# PART 5: Preparing the Results for Modelling\n",
    "\n",
    "In order to have a better understanding of the TF-IDF analysis done, we can prepare the results for modelling. The following code cells run the following:\n",
    "- Convert the variable `tfs`, which contains the actual results of the TF-IDF analysis, into a pandas DataFrame. `tfs.toarray()` is the data of the output in array form while its columns can be obtained using the `get_feature_names` method\n",
    "- Concatenate the newly created dataframe with `df_full_title` to get an association with the full title, along with the results of the TF-IDF. The new created will now have the following features/columns: [`full_title`, `bag_of_words_0`, `bag_of_words_1`,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d534ebc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10 republic</th>\n",
       "      <th>10 republic act</th>\n",
       "      <th>100 beds</th>\n",
       "      <th>1000 beds</th>\n",
       "      <th>10632 republic</th>\n",
       "      <th>10632 republic act</th>\n",
       "      <th>10656 republic</th>\n",
       "      <th>10656 republic act</th>\n",
       "      <th>10868 known</th>\n",
       "      <th>10868 known centenarians</th>\n",
       "      <th>...</th>\n",
       "      <th>zamboanga del norte</th>\n",
       "      <th>zamboanga del sur</th>\n",
       "      <th>zamboanga sibugay</th>\n",
       "      <th>zone appropriating</th>\n",
       "      <th>zone appropriating funds</th>\n",
       "      <th>zone authority</th>\n",
       "      <th>zone authority appropriating</th>\n",
       "      <th>zone freeport</th>\n",
       "      <th>zone providing</th>\n",
       "      <th>ꞌan act</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10 republic  10 republic act  100 beds  1000 beds  10632 republic  \\\n",
       "0          0.0              0.0       0.0        0.0             0.0   \n",
       "1          0.0              0.0       0.0        0.0             0.0   \n",
       "2          0.0              0.0       0.0        0.0             0.0   \n",
       "3          0.0              0.0       0.0        0.0             0.0   \n",
       "4          0.0              0.0       0.0        0.0             0.0   \n",
       "\n",
       "   10632 republic act  10656 republic  10656 republic act  10868 known  \\\n",
       "0                 0.0             0.0                 0.0          0.0   \n",
       "1                 0.0             0.0                 0.0          0.0   \n",
       "2                 0.0             0.0                 0.0          0.0   \n",
       "3                 0.0             0.0                 0.0          0.0   \n",
       "4                 0.0             0.0                 0.0          0.0   \n",
       "\n",
       "   10868 known centenarians  ...  zamboanga del norte  zamboanga del sur  \\\n",
       "0                       0.0  ...                  0.0                0.0   \n",
       "1                       0.0  ...                  0.0                0.0   \n",
       "2                       0.0  ...                  0.0                0.0   \n",
       "3                       0.0  ...                  0.0                0.0   \n",
       "4                       0.0  ...                  0.0                0.0   \n",
       "\n",
       "   zamboanga sibugay  zone appropriating  zone appropriating funds  \\\n",
       "0                0.0                 0.0                       0.0   \n",
       "1                0.0                 0.0                       0.0   \n",
       "2                0.0                 0.0                       0.0   \n",
       "3                0.0                 0.0                       0.0   \n",
       "4                0.0                 0.0                       0.0   \n",
       "\n",
       "   zone authority  zone authority appropriating  zone freeport  \\\n",
       "0             0.0                           0.0            0.0   \n",
       "1             0.0                           0.0            0.0   \n",
       "2             0.0                           0.0            0.0   \n",
       "3             0.0                           0.0            0.0   \n",
       "4             0.0                           0.0            0.0   \n",
       "\n",
       "   zone providing  ꞌan act  \n",
       "0             0.0      0.0  \n",
       "1             0.0      0.0  \n",
       "2             0.0      0.0  \n",
       "3             0.0      0.0  \n",
       "4             0.0      0.0  \n",
       "\n",
       "[5 rows x 3000 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(tfs.toarray(), columns = feature_names)\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6772422",
   "metadata": {},
   "source": [
    "1.5 mins runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "109ed0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([df, df1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d29cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not run this cell yet! We have to find ways to export this without reaching a 1 GB file size\n",
    "# final.to_csv('18th_hor_bills_final.csv', encoding='utf-8', chunksize=250000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077c802",
   "metadata": {},
   "source": [
    "# PART 6: The Actual Modelling Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c22d5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7254\n",
       "1    3586\n",
       "Name: approved, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['approved'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2737167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.drop(['ID', 'Full Title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06d2d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "small = shuffle(final)\n",
    "train_set, test_set, train_stat, test_stat = train_test_split(small.drop('approved', axis=1), small['approved'], test_size=1/7.0, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af21d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(train_set)\n",
    "train_set = scaler.transform(train_set)\n",
    "test_set = scaler.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d6cd56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=0.95)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=0.95)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=0.95)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca = PCA(.95)\n",
    "pca.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eee012bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.19360479e-01,  1.06842573e-01,  3.24372534e-02, ...,\n",
       "         1.04900825e-03, -4.17127604e-04, -1.10793483e-03],\n",
       "       [ 8.00527382e-03,  8.15722115e-03,  5.85734749e-03, ...,\n",
       "         1.01797087e-04, -2.58933515e-04, -2.79875973e-04],\n",
       "       [ 8.10777041e-03,  7.75174665e-03,  7.40371352e-03, ...,\n",
       "         3.39359745e-04,  8.56638779e-06,  1.91406444e-04],\n",
       "       ...,\n",
       "       [-5.45022284e-05,  7.51204586e-03,  2.70691919e-02, ...,\n",
       "         1.63755131e-02,  4.86929521e-02, -3.56172453e-03],\n",
       "       [ 8.96821406e-04,  6.28620778e-03,  3.26106406e-02, ...,\n",
       "         1.16554069e-02, -1.08798068e-02,  7.53664609e-03],\n",
       "       [-2.20666619e-04,  2.05483491e-03,  6.93360570e-03, ...,\n",
       "        -7.21227358e-04, -2.99473707e-02,  6.20806454e-04]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30639025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1329"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6376c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pca.transform(train_set)\n",
    "test_set = pca.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3d01c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='saga')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='saga')\n",
    "model.fit(train_set, train_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75d15811",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc50a904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive(TP)  =  360\n",
      "False Positive(FP) =  109\n",
      "True Negative(TN)  =  922\n",
      "False Negative(FN) =  158\n",
      "Accuracy of the binary classification = 0.828\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(test_stat, predictions)\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(test_stat, predictions).ravel()\n",
    "\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "\n",
    "accuracy =  (TP+TN) /(TP+FP+TN+FN)\n",
    "\n",
    "print('Accuracy of the binary classification = {:0.3f}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "413c54d42d867d78bc5693b88112002b4b75f03abc9fed1665b973f73c05d110"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
