{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe1551e",
   "metadata": {},
   "source": [
    "# CS 180 Machine Project Notebook [18th Congress Bills Analysis]\n",
    "\n",
    "This Python notebook visualizes the TF-IDF analysis and Machine Learning models done to the 18th Congress (House of Representatives) bills. The following procedures were made and are documented in this notebook:\n",
    "- Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "- MODEL 1: Principal Component Analysis (PCA) + Logistic Regression\n",
    "- MODEL 2: Incremental PCA + Support Vector Machines (SVM)\n",
    "\n",
    "This notebook was prepared by **GROUP 3 - *Blank Space***:\n",
    "- James Matthew Borines\n",
    "- Michael Benjamin Morco\n",
    "- Kyle Gabriel Reynoso\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "Make sure to have the following libraries installed on your machine before running the cells below to avoid any errors.\n",
    "- `pandas`\n",
    "- `nltk`\n",
    "- `tqdm`\n",
    "- `sklearn`\n",
    "\n",
    "All libraries were installed via `pip` using the command: `pip install <library-name>`. If you have `pip` installed in your machine, then you can easily install the following libraries using the command shown above.\n",
    "\n",
    "**NOTE**: Some of the code cells below have a running time of at least 30 minutes, with some even reaching an hour.\n",
    "\n",
    "## Outline of this Notebook\n",
    "[PART 1: Prepare the Dataframe](#part1) <br/>\n",
    "[PART 2: Load the Stop Words and Extract the Bill Title](#part2) <br/>\n",
    "[PART 3: Perform TF-IDF](#part3) <br/>\n",
    "[PART 4: Preparing the Results for Modelling](#part4) <br/>\n",
    "[PART 5: The Actual Modelling Part](#part5) <br/>\n",
    "&emsp;[PART 5.1: PCA + Logistic Regression](#part51) <br/>\n",
    "&emsp;[PART 5.2: Incremental PCA + SVM](#part52) <br/>\n",
    "&emsp;[PART 5.3: Analysis of Two Models](#part53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8f338",
   "metadata": {},
   "source": [
    "<a id = \"part1\"></a>\n",
    "## PART 1: Prepare the Dataframe\n",
    "\n",
    "For this part, we prepare and load the *18th House of Representatives Bills Dataset*, which was preprocessed. The dataset contains the following features:\n",
    "- Important Characteristics of a House Bill (e.g. ID, Full Title, Number of Authors, etc.)\n",
    "- One-Hot Encoding of the status of the *Significance* and *Primary Referral*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a4efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('18th_hor_bills_dataset_with_status.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861f3d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10840, 279)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06466ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                                                                HB00002\n",
       "Full Title                              AN ACT CREATING THE DEPARTMENT OF OVERSEAS FIL...\n",
       "Author Count                                                                            3\n",
       "is_partylist                                                                            0\n",
       "party_1-pacman                                                                          0\n",
       "                                                              ...                        \n",
       "ref_defeat_covid-19_ad-hoc_committee                                                    0\n",
       "ref_the_whole_house                                                                     0\n",
       "ref_mindanao_affairs                                                                    0\n",
       "ref_west_philippine_sea                                                                 0\n",
       "approved                                                                                1\n",
       "Name: 1, Length: 279, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load a particular instance of the dataset\n",
    "df.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cf6cb",
   "metadata": {},
   "source": [
    "<a id = \"part2\"></a>\n",
    "# PART 2: Load the Stop Words and Extract the Bill Title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067d009",
   "metadata": {},
   "source": [
    "A list of English stopwords can be loaded using the `nltk` library. We can download the `stopwords` first before loading it to a variable `stops`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a232892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this cell only if 'stopwords' has not been downloaded or if the succeeding cell throws an error\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e76d652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f565e54",
   "metadata": {},
   "source": [
    "Extract the 'Full Title' of the bills first into a separate dataset to prepare it for TF-IDF analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3302a736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    AN ACT INSTITUTIONALIZING A NATIONAL VALUES, E...\n",
       "1    AN ACT CREATING THE DEPARTMENT OF OVERSEAS FIL...\n",
       "2    AN ACT PROVIDING FOR A NATIONAL PROGRAM TO SUP...\n",
       "3    AN ACT CREATING THE EMERGENCY RESPONSE DEPARTM...\n",
       "4    AN ACT INSTITUTIONALIZING MICROFINANCE PROGRAM...\n",
       "Name: Full Title, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_title = df['Full Title'].copy(deep = True)\n",
    "df_full_title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24581192",
   "metadata": {},
   "source": [
    "As we prepare the list of bills for TF-IDF, we can now remove the stop words and punctuations from each of the full titles.\n",
    "\n",
    "Let us define a function `remove_punctuation` that removes the punctuations and retains the letters from each of the token, and test the function over a set of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d95e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(token):\n",
    "    import string\n",
    "    return token.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b35bb9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shout out to my ex you are really quite the man you made my heart break and that made me who Im'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str_1 = 'Shout out to my ex, you are really quite the man; you made my heart break, and that made me who I\\'m'\n",
    "remove_punctuation(test_str_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb77dc",
   "metadata": {},
   "source": [
    "Let us define a function `remove_stop_words` that does the following:\n",
    "<ul>\n",
    "    <li> Split the bill into a list of tokens or words </li>\n",
    "    <li> If a stop word, as indicated in variable a defined above, is included, remove them from the list of tokens </li>\n",
    "    <li> Combine the remaining words into a string separated by spaces </li>\n",
    "</ul>\n",
    "We can then test the newly created function to some test entries, before finally iterating it over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aae8d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(bill_string, stopwords):\n",
    "    lis = bill_string.split() #split the string according to spaces\n",
    "    to_return = [] #define a new list of words\n",
    "    \n",
    "    for i in lis:\n",
    "        if i.lower() not in stopwords:\n",
    "            to_return.append(i)\n",
    "    \n",
    "    return remove_punctuation(\" \".join(to_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a0f4c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACT CREATING DEPARTMENT OVERSEAS FILIPINO WORKERS OFW FOREIGN EMPLOYMENT DEFINING POWERS FUNCTIONS APPROPRIATING FUNDS THEREFOR RATIONALIZING ORGANIZATION FUNCTIONS GOVERNMENT AGENCIES RELATED MIGRATION PURPOSES'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stop_words(df_full_title.iloc[1], stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c34557e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACT PROVIDING COMPREHENSIVE CIVIL REGISTRATION SYSTEM'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stop_words(df_full_title.iloc[103], stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cffb20",
   "metadata": {},
   "source": [
    "Upon running the function initially over all bills, the function threw an error over at entry 9278, which corresponds to 'HB09290'. The full title of this house bill is an empty string. Upon cross-checking the data with the website, it was found out that the website was not able to include the full title of the actual bill. Fortunately, the full title of this bill can be accessed using the link: https://hrep-website.s3.ap-southeast-1.amazonaws.com/legisdocs/basic_18/HB09290.pdf\n",
    "\n",
    "The name of the bill will then be hardcoded to its corresponding entry, 9278, before running the function over all bills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04027219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AN ACT TO IMPROVE ACCESS TO PRESCHOOL, PRIMARY, AND SECONDARY EDUCATION OF HOMELESS CHILDREN AND YOUTH'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_title.iloc[9278] = \"AN ACT TO IMPROVE ACCESS TO PRESCHOOL, PRIMARY, AND SECONDARY EDUCATION OF HOMELESS CHILDREN AND YOUTH\"\n",
    "df_full_title.iloc[9278]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9101a5df",
   "metadata": {},
   "source": [
    "The function will be applied to all bills, and test the new data by sampling entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78486726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a52856e314427db77ecec6813e1ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for i in tqdm(range(len(df_full_title))):\n",
    "    df_full_title.iloc[i] = remove_stop_words(df_full_title.iloc[i], stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23a80490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACT CONVERTING DARAGAPILAR DIVERSION ROAD MUNICIPALITY DARAGA PROVINCE ALBAY MUNICIPALITY PILAR PROVINCE SORSOGON NATIONAL ROAD APPROPRIATING FUNDS THEREFOR'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_title.iloc[10839]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50ed187c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACT ESTABLISHING BENHAM RISE RESEARCH DEVELOPMENT INSTITUTE PROVIDING FUNDS THEREFOR PURPOSES'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_title.iloc[34]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2487ab4b",
   "metadata": {},
   "source": [
    "<a id = \"part3\"></a>\n",
    "# PART 3: Perform TF-IDF\n",
    "\n",
    "For this portion, we will be using the `TfidfVectorizer` function from `sklearn.feature_extraction.text`. This function needs the following input parameters:\n",
    "- `max_features`: Maximum number of n-grams to be considered as features\n",
    "- `stop-words`\n",
    "- `ngram_range`: The range of n-grams, given by the `lower_limit` and `upper_limit`\n",
    "\n",
    "Only certain n-grams were considered upon performing TF-IDF since the dataframe will have a size of 17.1 GiB when the full vocabulary (~211000 features) is considered for TF-IDF, which cannot be processed by machines with 16 GB of memory. We can customize the `ngram_range` and the `max_features` fields to customize the results of the TF-IDF analysis. We can then manipulate the following variables:\n",
    "- `lower_limit`: Minimum number of n-grams to be considered\n",
    "- `upper_limit`: Maximum number of n-grams to be considered\n",
    "- `no_words`: Number of words to be considered as features for TF-IDF. The top `no_words` words according to frequency will only be considered. If your machine has more than 16 GB of RAM, you may want to consider omitting `no_words` and the `max_features` parameter of `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a2102e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can tweak lower_limit and upper_limit depending on the number of n-grams that you want to be extracted\n",
    "lower_limit = 2\n",
    "upper_limit = 4\n",
    "no_words = 40000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344113c",
   "metadata": {},
   "source": [
    "Perform the actual TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf6d50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features = no_words, stop_words = 'english', ngram_range=(lower_limit, upper_limit))\n",
    "tfs = tfidf.fit_transform(df_full_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199dfce6",
   "metadata": {},
   "source": [
    "<a id = \"part4\"></a>\n",
    "# PART 4: Preparing the Results for Modelling\n",
    "\n",
    "After performing TF-IDF, we can now prepare the original dataframe and the TF-IDF results dataframe for modelling. The following code cells run the following:\n",
    "- Convert the variable `tfs`, which contains the actual results of the TF-IDF analysis, into a pandas DataFrame. `tfs.toarray()` is the data of the output in array form while its columns can be obtained using the `get_feature_names` method\n",
    "- Concatenate the newly created dataframe with `df_full_title` to get an association with the full title, along with the results of the TF-IDF. The new created will now have the following features/columns: [`full_title`, `bag_of_words_0`, `bag_of_words_1`,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c964aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names_out() #get the bag of words used in TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d534ebc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10 11</th>\n",
       "      <th>10 11 12</th>\n",
       "      <th>10 11 12 presidential</th>\n",
       "      <th>10 12</th>\n",
       "      <th>10 12 amending</th>\n",
       "      <th>10 12 amending purpose</th>\n",
       "      <th>10 50</th>\n",
       "      <th>10 50 beds</th>\n",
       "      <th>10 50 beds appropriating</th>\n",
       "      <th>10 additional</th>\n",
       "      <th>...</th>\n",
       "      <th>ꞌthe revised</th>\n",
       "      <th>ꞌtobacco regulation</th>\n",
       "      <th>ꞌtobacco regulation act</th>\n",
       "      <th>ꞌtobacco regulation act 2003ꞌ</th>\n",
       "      <th>ꞌuniversal health</th>\n",
       "      <th>ꞌuniversal health care</th>\n",
       "      <th>ꞌuniversal health care actꞌ</th>\n",
       "      <th>ꞌurban development</th>\n",
       "      <th>ꞌurban development housing</th>\n",
       "      <th>ꞌurban development housing act</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10 11  10 11 12  10 11 12 presidential  10 12  10 12 amending  \\\n",
       "0    0.0       0.0                    0.0    0.0             0.0   \n",
       "1    0.0       0.0                    0.0    0.0             0.0   \n",
       "2    0.0       0.0                    0.0    0.0             0.0   \n",
       "3    0.0       0.0                    0.0    0.0             0.0   \n",
       "4    0.0       0.0                    0.0    0.0             0.0   \n",
       "\n",
       "   10 12 amending purpose  10 50  10 50 beds  10 50 beds appropriating  \\\n",
       "0                     0.0    0.0         0.0                       0.0   \n",
       "1                     0.0    0.0         0.0                       0.0   \n",
       "2                     0.0    0.0         0.0                       0.0   \n",
       "3                     0.0    0.0         0.0                       0.0   \n",
       "4                     0.0    0.0         0.0                       0.0   \n",
       "\n",
       "   10 additional  ...  ꞌthe revised  ꞌtobacco regulation  \\\n",
       "0            0.0  ...           0.0                  0.0   \n",
       "1            0.0  ...           0.0                  0.0   \n",
       "2            0.0  ...           0.0                  0.0   \n",
       "3            0.0  ...           0.0                  0.0   \n",
       "4            0.0  ...           0.0                  0.0   \n",
       "\n",
       "   ꞌtobacco regulation act  ꞌtobacco regulation act 2003ꞌ  ꞌuniversal health  \\\n",
       "0                      0.0                            0.0                0.0   \n",
       "1                      0.0                            0.0                0.0   \n",
       "2                      0.0                            0.0                0.0   \n",
       "3                      0.0                            0.0                0.0   \n",
       "4                      0.0                            0.0                0.0   \n",
       "\n",
       "   ꞌuniversal health care  ꞌuniversal health care actꞌ  ꞌurban development  \\\n",
       "0                     0.0                          0.0                 0.0   \n",
       "1                     0.0                          0.0                 0.0   \n",
       "2                     0.0                          0.0                 0.0   \n",
       "3                     0.0                          0.0                 0.0   \n",
       "4                     0.0                          0.0                 0.0   \n",
       "\n",
       "   ꞌurban development housing  ꞌurban development housing act  \n",
       "0                         0.0                             0.0  \n",
       "1                         0.0                             0.0  \n",
       "2                         0.0                             0.0  \n",
       "3                         0.0                             0.0  \n",
       "4                         0.0                             0.0  \n",
       "\n",
       "[5 rows x 40000 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(tfs.toarray(), columns = feature_names)\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6772422",
   "metadata": {},
   "source": [
    "`df1` now contains the TF-IDF results of all bills, with each column pertaining to a $n$-gram that was used as a *vocabulary*. We then concatenate this dataframe to the existing dataframe `df`. Runtime for the concatenate command depends on the speed of the machine, but its running time on average is 1.5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "109ed0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([df, df1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f352143",
   "metadata": {},
   "source": [
    "The `final` dataframe now contains all the features from the preprocessing aspect and the TF-IDF analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077c802",
   "metadata": {},
   "source": [
    "<a id = \"part5\"></a>\n",
    "# PART 5: The Actual Modelling Part\n",
    "\n",
    "The modelling phase can be divided into two parts:\n",
    "1. Principal Component Analysis (PCA) + Logistic Regression\n",
    "2. Incremental PCA + Support Vector Machines\n",
    "\n",
    "Before modelling, the `final` dataframe will be preprocessed first by:\n",
    "- Shuffling the dataset to avoid any bias that may come along the process\n",
    "- Splitting our dataset into training and test sets using `train_test_split` from `sklearn.model_selection`\n",
    "- Scaling our training and test set using `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c22d5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7254\n",
       "1    3586\n",
       "Name: approved, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['approved'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c0a522",
   "metadata": {},
   "source": [
    "Based on the value counts of the `final` dataframe, there are:\n",
    "- 7254 bills that are treated as \"Not Approved\" (66.92%)\n",
    "- 3586 bills that are treated as \"Approved\" (33.08%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2737167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the ID and Full Title fields from the final dataframe\n",
    "final = final.drop(['ID', 'Full Title'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659aa239",
   "metadata": {},
   "source": [
    "We then split our dataset into training and test sets, and scale our data using `StandardScaler`.\n",
    "- We divide our dataset into 80:20 ratio: 80% for the training set and the remaining 20% for the test set\n",
    "- To ensure that the ratio of Unapproved to Approved bills are kept constant throughout the split, we stratify our split based on the `approved` feature.\n",
    "- `train_set` and `train_stat` contain the features and the status output of the bills for the training portion of the dataset, respectively.\n",
    "- `test_set` and `test_stat` contain the features and the status output of the bills for the test portion of the dataset, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06d2d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "small = shuffle(final)\n",
    "train_set, test_set, train_stat, test_stat = train_test_split(small.drop('approved', axis=1), \n",
    "                                                              small['approved'], test_size=1/5,\n",
    "                                                             stratify = small['approved'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af21d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(train_set)\n",
    "train_set = scaler.transform(train_set)\n",
    "test_set = scaler.transform(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024715db",
   "metadata": {},
   "source": [
    "Upon running `value_counts()` for both training and test sets,\n",
    "- For the training set, the number of Unapproved is 5803 (66.92%) while the number of Approved is 2869 (33.08%)\n",
    "- For the test set, the number of Unapproved is 1451 (66.92%) while the number of Approved is 717 (33.08%)\n",
    "Both the training and test sets are consistent in terms of the ratio of *Unapproved* to *Approved* bills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2d3598d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5803\n",
       "1    2869\n",
       "Name: approved, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "912cc27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1451\n",
       "1     717\n",
       "Name: approved, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0691c5a7",
   "metadata": {},
   "source": [
    "<a id = \"part51\"></a>\n",
    "## PART 5.1: Principal Component Analysis + Logistic Regression\n",
    "\n",
    "For this part, given 3000 maximum features and with `ngram_range` of $[2, 4]$, principal component analysis (PCA) from `sklearn.decomposition` was used to reduce the dimensionality of the features, and then run a *Logistic Regression* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d6cd56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=0.95)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=0.95)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=0.95)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# We then fit our training set using PCA with 95% variability\n",
    "pca = PCA(.95)\n",
    "pca.fit(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12a171",
   "metadata": {},
   "source": [
    "We then transform both our training and testing set using the `transform` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30639025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1142"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#determine new number of components/features after fitting the dataset\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6376c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform both training and test set according to PCA\n",
    "train_set = pca.transform(train_set)\n",
    "test_set = pca.transform(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf942d",
   "metadata": {},
   "source": [
    "We import `LogisticRegressionCV` from `sklearn.linear_model` and set up an instance under the variable `model`. We can now feed our training set, represented by `train_set` and `train_stat` to fit into the logistic regression model. However, the code cell block below was commented out since it will take a long time to re-run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3d01c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegressionCV(cv=5, max_iter=5000, n_jobs=4, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegressionCV</label><div class=\"sk-toggleable__content\"><pre>LogisticRegressionCV(cv=5, max_iter=5000, n_jobs=4, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegressionCV(cv=5, max_iter=5000, n_jobs=4, solver='saga')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.linear_model import LogisticRegressionCV\n",
    "# model = LogisticRegressionCV(solver='saga', cv=5, max_iter=5000, n_jobs=4)\n",
    "# model.fit(train_set, train_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc56a6c",
   "metadata": {},
   "source": [
    "Instead of retraining, you can simply load the pickled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44e9848e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8242619926199262"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "model = pickle.load(open('logregcvmodel.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b2235b",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75d15811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8242619926199262"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(test_set,test_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fcc9385c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([[0.78847262, 0.82074928, 0.82074928, 0.81440922, 0.81095101,\n",
       "         0.80864553, 0.80806916, 0.80806916, 0.80806916, 0.80806916],\n",
       "        [0.78674352, 0.82536023, 0.82478386, 0.81325648, 0.80634006,\n",
       "         0.80691643, 0.80691643, 0.80691643, 0.80691643, 0.80691643],\n",
       "        [0.78373702, 0.81141869, 0.80622837, 0.80046136, 0.79873126,\n",
       "         0.79930796, 0.79757785, 0.79757785, 0.79757785, 0.79757785],\n",
       "        [0.79296424, 0.82929642, 0.82525952, 0.80968858, 0.80103806,\n",
       "         0.79930796, 0.79930796, 0.79930796, 0.79930796, 0.79930796],\n",
       "        [0.77508651, 0.81314879, 0.81314879, 0.81545559, 0.8160323 ,\n",
       "         0.81372549, 0.81545559, 0.81430219, 0.81430219, 0.81430219]])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be6e3c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cc50a904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive(TP)  =  445\n",
      "False Positive(FP) =  93\n",
      "True Negative(TN)  =  1342\n",
      "False Negative(FN) =  288\n",
      "Accuracy of the binary classification = 0.824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(test_stat, predictions)\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(test_stat, predictions).ravel()\n",
    "\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "\n",
    "accuracy =  (TP+TN) /(TP+FP+TN+FN)\n",
    "\n",
    "print('Accuracy of the binary classification = {:0.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6eec24",
   "metadata": {},
   "source": [
    "The precision can also be computed as $\\frac{TP}{TP + FP}: \\frac{445}{445 + 93} = $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "43b9e0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8667    0\n",
       "3036    0\n",
       "4449    0\n",
       "5664    1\n",
       "8388    0\n",
       "       ..\n",
       "9161    0\n",
       "6096    0\n",
       "4860    0\n",
       "7379    0\n",
       "7547    0\n",
       "Name: approved, Length: 2168, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "80eec0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08129334, 0.20051739, 0.14112781, ..., 0.09307892, 0.73290162,\n",
       "       0.2420878 ])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(test_set)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "54439ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12771165185479205"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "brier_score = brier_score_loss(test_stat, model.predict_proba(test_set)[:,1])\n",
    "brier_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0b035fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4052578552509612"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "loss = log_loss(test_stat, model.predict_proba(test_set)[:,1])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "38d4943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('logregcvmodel.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ed148",
   "metadata": {},
   "source": [
    "<a id = \"part52\"></a>\n",
    "## PART 5.2: Incremental PCA + Support Vector Machines\n",
    "\n",
    "For this part, we will try out Incremental PCA and SVM to predict the status of the 18th Congress bills. The results and performance of the SVM model will then be compared to [PART 5.1](#part51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "226198f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IncrementalPCA()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IncrementalPCA</label><div class=\"sk-toggleable__content\"><pre>IncrementalPCA()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "IncrementalPCA()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "ipca = IncrementalPCA()\n",
    "ipca.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1d0715c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8672"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#determine new number of components after applying Incremental PCA\n",
    "ipca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cea7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the features training set and test set using transform method\n",
    "train_set = ipca.transform(train_set)\n",
    "test_set = ipca.transform(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c159b59b",
   "metadata": {},
   "source": [
    "After conducting Incremental PCA, we could now do Support Vector Machines using `sklearn.svm.SVC`\n",
    "\n",
    "**NOTE**: The estimated running time for this model is ~1 hour. This is to be expected since the output of SVM is being converted to probability estimates through [Platt scaling](https://scikit-learn.org/stable/modules/svm.html#scores-probabilities). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1f0d764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "classifier_instance = svm.SVC(kernel = \"rbf\", probability = True) #create instance of SVM\n",
    "classifier_instance.fit(train_set, train_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "056e2360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8805350553505535"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_instance.score(test_set, test_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf080642",
   "metadata": {},
   "source": [
    "After fitting the training set into the model, we can now determine the model output for our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0eef8531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "proba = classifier_instance.predict_proba(test_set)\n",
    "svm_predict = [int(x > 0.4) for x in proba[:, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb77b5",
   "metadata": {},
   "source": [
    "### Performance Measurement\n",
    "\n",
    "The performance measurement of SVM can be measured through its accuracy, precision, and recall.\n",
    "- A confusion matrix was generated that unravels the True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN) results upon feeding the test set to the model.\n",
    "- Accuracy is then computed as: $\\frac{TP + TN}{TP + FP + TN + FN}$\n",
    "- Precision is then computed as: $\\frac{TP}{TP + FP}$\n",
    "- Recall is then computed as: $\\frac{TP}{TP + FN}$\n",
    "\n",
    "The performance measurement of SVM can also be measured using *Brier Score* and *Log Loss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2eaa7d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive(TP)  =  579\n",
      "False Positive(FP) =  123\n",
      "True Negative(TN)  =  1328\n",
      "False Negative(FN) =  138\n",
      "Accuracy of the SVM binary classification = 87.961%\n",
      "Precision = 82.479%\n",
      "Recall = 80.753%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(test_stat, svm_predict).ravel() #construct confusion matrix\n",
    "\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "\n",
    "accuracy =  (TP+TN) /(TP+FP+TN+FN) * 100\n",
    "precision = TP/(TP + FP) * 100\n",
    "recall = TP/(TP + FN) * 100\n",
    "\n",
    "print('Accuracy of the SVM binary classification = {:0.3f}%'.format(accuracy))\n",
    "print('Precision = {:0.3f}%'.format(precision))\n",
    "print('Recall = {:0.3f}%'.format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aebe37cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09422114184656694"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#determine brier score\n",
    "from sklearn.metrics import brier_score_loss\n",
    "brier_score = brier_score_loss(test_stat, classifier_instance.predict_proba(test_set)[:,1])\n",
    "brier_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfcea20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33058951411280874"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#determine log loss score\n",
    "from sklearn.metrics import log_loss\n",
    "loss = log_loss(test_stat, proba)\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "413c54d42d867d78bc5693b88112002b4b75f03abc9fed1665b973f73c05d110"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
